{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from `SMS Spam Collection v. 1` described as:\n",
    "\n",
    "> a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-encoded messages, tagged according being legitimate (ham) or spam.\n",
    "\n",
    "([source](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load useful libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\n",
    "    \"data/SMSSpamCollection.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"target\", \"text\"],\n",
    ")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encoding target variable\n",
    "data[\"target\"] = np.where(data[\"target\"] == \"spam\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm in school now n i'll be in da lab doing so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0</td>\n",
       "      <td>Can you open the door?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>0</td>\n",
       "      <td>Yup ok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "5170       0  I'm in school now n i'll be in da lab doing so...\n",
       "573        0                             Can you open the door?\n",
       "4963       0                                          Yup ok..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample of our data\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  5572 non-null   int32 \n",
      " 1   text    5572 non-null   object\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 65.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5572 instances of 2 variables.\n",
      "It contains 747 spam messages (13.4% of all).\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset contains {} instances of {} variables.\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "print(\n",
    "    \"It contains {} spam messages ({:.1%} of all).\".format(\n",
    "        data[data.target == 1].shape[0],\n",
    "        data[data.target == 1].shape[0] / data.shape[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       target                                               text\n",
       "0          0  Go until jurong point, crazy.. Available only ...\n",
       "1          0                      Ok lar... Joking wif u oni...\n",
       "2          1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3          0  U dun say so early hor... U c already then say...\n",
       "4          0  Nah I don't think he goes to usf, he lives aro...\n",
       "...      ...                                                ...\n",
       "5567       1  This is the 2nd time we have tried 2 contact u...\n",
       "5568       0               Will ü b going to esplanade fr home?\n",
       "5569       0  Pity, * was in mood for that. So...any other s...\n",
       "5570       0  The guy did some bitching but I acted like i'd...\n",
       "5571       0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>1</td>\n",
       "      <td>Want a new Video Phone? 750 anytime any networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>1</td>\n",
       "      <td>Save money on wedding lingerie at www.bridal.p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "4156       1  Want a new Video Phone? 750 anytime any networ...\n",
       "3443       1  Save money on wedding lingerie at www.bridal.p..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[data.target==1].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of spam SMS: \n",
      "    PRIVATE! Your 2003 Account Statement for 07973788240 shows 800 un-redeemed S. I. M. points. Call 08715203649 Identifier Code: 40533 Expires 31/10/04\n",
      "    For ur chance to win a £250 wkly shopping spree TXT: SHOP to 80878. T's&C's www.txt-2-shop.com custcare 08715705022, 1x150p/wk\n",
      "\n",
      "Examples of non-spam SMS: \n",
      "    1) Go to write msg 2) Put on Dictionary mode 3)Cover the screen with hand, 4)Press  &lt;#&gt; . 5)Gently remove Ur hand.. Its interesting..:)\n",
      "    Jesus christ bitch I'm trying to give you drugs answer your fucking phone\n"
     ]
    }
   ],
   "source": [
    "## Printing random samples of text from both the classes i.e. Spam and non-Spam\n",
    "print(\n",
    "    \"Examples of spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"\\nExamples of non-spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classification\n",
    "\n",
    "We will here build a \"vanilla\" classifier, without pouring too many thoughts about what the actual messages, spam or not, look like. To improve your model you can of course have a closer look and investigate the data more in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"text\"], data[\"target\"], random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872     Its going good...no problem..but still need li...\n",
       "831     U have a secret admirer. REVEAL who thinks U R...\n",
       "1273                                                Ok...\n",
       "3314    Huh... Hyde park not in mel ah, opps, got conf...\n",
       "4929    Just hopeing that wasn‘t too pissed up to reme...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "As you have seen, our X variable is just plain text == a string. No classifier can handle it, so we need to make the text accessible to the model. Therefore, we can transform the text so that each word is a separate feature and we count how many times that word occurs in the SMS. We can do this with the scikit-learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  It will convert our text and return a sparse matrix. The vocabulary space of an English text is quite large, while in an SMS you will use only a small subset of words. Therefore saving this feature matrix as a sparse matrix will save memory space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized:  <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 55484 stored elements and shape (4179, 7546)>\n",
      "  Coords\tValues\n",
      "  (0, 915)\t1\n",
      "  (0, 1526)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 2630)\t1\n",
      "  (0, 3090)\t1\n",
      "  (0, 3100)\t1\n",
      "  (0, 3676)\t1\n",
      "  (0, 4049)\t1\n",
      "  (0, 4593)\t1\n",
      "  (0, 4663)\t1\n",
      "  (0, 5294)\t1\n",
      "  (0, 6315)\t1\n",
      "  (0, 6764)\t1\n",
      "  (0, 6964)\t1\n",
      "  (0, 7133)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 216)\t1\n",
      "  (1, 532)\t1\n",
      "  (1, 801)\t1\n",
      "  (1, 1553)\t1\n",
      "  (1, 1598)\t1\n",
      "  (1, 2055)\t1\n",
      "  (1, 3267)\t1\n",
      "  (1, 4493)\t1\n",
      "  (1, 4835)\t1\n",
      "  :\t:\n",
      "  (4176, 6904)\t1\n",
      "  (4176, 6916)\t1\n",
      "  (4176, 7019)\t1\n",
      "  (4176, 7333)\t1\n",
      "  (4176, 7367)\t2\n",
      "  (4176, 7449)\t1\n",
      "  (4177, 1575)\t1\n",
      "  (4177, 2422)\t1\n",
      "  (4177, 3081)\t1\n",
      "  (4177, 3424)\t1\n",
      "  (4177, 3505)\t1\n",
      "  (4177, 3551)\t1\n",
      "  (4177, 4306)\t1\n",
      "  (4177, 4538)\t1\n",
      "  (4177, 5739)\t1\n",
      "  (4177, 7228)\t2\n",
      "  (4178, 2121)\t1\n",
      "  (4178, 2717)\t1\n",
      "  (4178, 3040)\t1\n",
      "  (4178, 3677)\t1\n",
      "  (4178, 4410)\t1\n",
      "  (4178, 5901)\t1\n",
      "  (4178, 6070)\t1\n",
      "  (4178, 6644)\t1\n",
      "  (4178, 6771)\t1\n"
     ]
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(\"X_train_vectorized: \", X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (4179,)\n",
      "Vocabulary length = 7546\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in 4179 messages we found 7546 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000', 0),\n",
       " ('10', 1),\n",
       " ('100', 2),\n",
       " ('1000', 3),\n",
       " ('10p', 4),\n",
       " ('150p', 5),\n",
       " ('150ppm', 6),\n",
       " ('16', 7),\n",
       " ('18', 8),\n",
       " ('1st', 9),\n",
       " ('2000', 10),\n",
       " ('250', 11),\n",
       " ('2nd', 12),\n",
       " ('50', 13),\n",
       " ('500', 14),\n",
       " ('5000', 15),\n",
       " ('800', 16),\n",
       " ('able', 17),\n",
       " ('about', 18),\n",
       " ('abt', 19),\n",
       " ('account', 20),\n",
       " ('actually', 21),\n",
       " ('address', 22),\n",
       " ('after', 23),\n",
       " ('afternoon', 24),\n",
       " ('again', 25),\n",
       " ('ah', 26),\n",
       " ('aight', 27),\n",
       " ('all', 28),\n",
       " ('alone', 29),\n",
       " ('already', 30),\n",
       " ('alright', 31),\n",
       " ('also', 32),\n",
       " ('always', 33),\n",
       " ('am', 34),\n",
       " ('amp', 35),\n",
       " ('an', 36),\n",
       " ('and', 37),\n",
       " ('another', 38),\n",
       " ('answer', 39),\n",
       " ('any', 40),\n",
       " ('anything', 41),\n",
       " ('anyway', 42),\n",
       " ('apply', 43),\n",
       " ('ard', 44),\n",
       " ('are', 45),\n",
       " ('around', 46),\n",
       " ('as', 47),\n",
       " ('ask', 48),\n",
       " ('asked', 49),\n",
       " ('at', 50),\n",
       " ('attempt', 51),\n",
       " ('await', 52),\n",
       " ('awarded', 53),\n",
       " ('away', 54),\n",
       " ('babe', 55),\n",
       " ('baby', 56),\n",
       " ('back', 57),\n",
       " ('bad', 58),\n",
       " ('be', 59),\n",
       " ('beautiful', 60),\n",
       " ('because', 61),\n",
       " ('bed', 62),\n",
       " ('been', 63),\n",
       " ('before', 64),\n",
       " ('being', 65),\n",
       " ('believe', 66),\n",
       " ('best', 67),\n",
       " ('better', 68),\n",
       " ('between', 69),\n",
       " ('big', 70),\n",
       " ('birthday', 71),\n",
       " ('bit', 72),\n",
       " ('bonus', 73),\n",
       " ('book', 74),\n",
       " ('bored', 75),\n",
       " ('box', 76),\n",
       " ('boy', 77),\n",
       " ('bring', 78),\n",
       " ('bt', 79),\n",
       " ('bus', 80),\n",
       " ('busy', 81),\n",
       " ('but', 82),\n",
       " ('buy', 83),\n",
       " ('by', 84),\n",
       " ('call', 85),\n",
       " ('called', 86),\n",
       " ('calling', 87),\n",
       " ('calls', 88),\n",
       " ('came', 89),\n",
       " ('camera', 90),\n",
       " ('can', 91),\n",
       " ('cant', 92),\n",
       " ('car', 93),\n",
       " ('card', 94),\n",
       " ('care', 95),\n",
       " ('carlos', 96),\n",
       " ('cash', 97),\n",
       " ('chance', 98),\n",
       " ('chat', 99),\n",
       " ('check', 100),\n",
       " ('choose', 101),\n",
       " ('claim', 102),\n",
       " ('class', 103),\n",
       " ('close', 104),\n",
       " ('co', 105),\n",
       " ('code', 106),\n",
       " ('collect', 107),\n",
       " ('collection', 108),\n",
       " ('com', 109),\n",
       " ('come', 110),\n",
       " ('comes', 111),\n",
       " ('coming', 112),\n",
       " ('contact', 113),\n",
       " ('cool', 114),\n",
       " ('cos', 115),\n",
       " ('cost', 116),\n",
       " ('could', 117),\n",
       " ('coz', 118),\n",
       " ('cs', 119)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our vocabulary list (sorted alphabetically)\n",
    "# Does it look like you expected?\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# We can also print the newly created feature matrix\n",
    "# Note: you see its a sparse matrix with many 0 values. \n",
    "# with .toarray() the compressed sparse matrix form is converted to a normal numpy array\n",
    "print(X_train_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our first model with the vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.985\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "predict_probab = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which words have the highest and lowest coefficients.\n",
    "\n",
    "Think back to the sigmoid function (logistic function). \n",
    "What class are observations assigned to if they contain words with high coefficients?  And to which class are they assigned if they contain words with high negative coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'amp' 'right']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'chat' 'reply' 'new' 'won' 'stop']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model (from lowest to highest values)\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of our first model was already pretty good (~0.95). Let's see if we can improve this with another transformation of our data. Therefore, we will test the TF-IDF transformation next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF is short for **Term Frequency - Inverse Document Frequency**. \n",
    "\n",
    "It measure how important a word is to a document in a set of texts (in our case all SMS we collected). A frequent word in a document that is also frequent in the corpus is less important to a document than a frequent word in a document that is not frequent in the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000', 10),\n",
       " ('250', 11),\n",
       " ('2nd', 12),\n",
       " ('50', 13),\n",
       " ('500', 14),\n",
       " ('5000', 15),\n",
       " ('800', 16),\n",
       " ('able', 17),\n",
       " ('about', 18),\n",
       " ('abt', 19),\n",
       " ('account', 20),\n",
       " ('actually', 21),\n",
       " ('address', 22),\n",
       " ('after', 23),\n",
       " ('afternoon', 24),\n",
       " ('again', 25),\n",
       " ('ah', 26),\n",
       " ('aight', 27),\n",
       " ('all', 28),\n",
       " ('alone', 29)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 15\n",
    "# This means a word should have been used in at least 15 SMS \n",
    "vect = TfidfVectorizer(min_df=15).fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# let's look of some of the words gathered with this method\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in more than 15 text messages\n",
    "len(sorted(vect.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which words created the largest tfidf values for the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['2000' 'weekly' 'rate' '16' 'sae' 'await' 'vouchers' 'guaranteed' '1000'\n",
      " 'collection']\n",
      "\n",
      "Largest tfidf: \n",
      "['yup' 'with' 'sure' 'babe' 'heart' 'he' 'thank' 'why' 'happy' 'thanx']\n"
     ]
    }
   ],
   "source": [
    "# save all feature names == words in an array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#sort for the column names according to highest tfidf value in the column\n",
    "sorted_tfidf_index = X_train_vectorized.toarray().max(0).argsort()\n",
    "\n",
    "# print words with highest and lowest tfidf values\n",
    "print(\"Smallest tfidf:\\n{}\\n\".format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print(\"Largest tfidf: \\n{}\".format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new features with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.988\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 577 features out of 7546 (7546 different words were in our training texts), we still get a high value for the AUC score.\n",
    "Feel free to test different values for the minimum document frequency for the tf-idf vectorizer and see how this affects the model.\n",
    "\n",
    "Again, we can look at the coefficients of our new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'that' 'ok' 'later' 'da' 'how']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'call' 'text' 'free' 'stop' 'uk' 'claim' 'www' 'reply' '150p']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a lot of overlap in the features that received the highest and lowest coefficients compared to the previous model; regardless of how we convert our text into features, these words seem to be important for classifying spam with logistic regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data can be more preprocessed before being used as features in a model. We will first use stemming as an approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming reduces a word to its stem. The result is less readable to humans, but makes the text more comparable across observations.\n",
    "\n",
    "For example, the words \"consult\", \"consultant\", \"consulting\", \" consultative\", \"consultants\" have the same stem **\"consult \"**.\n",
    "\n",
    "We will now add stemming as a preprocessing step to our workflow. The nltk PorterStemmer will generate the stems of the words. These features will be used in the CountVectorizer to create a matrix with the number of features (stemmed words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# tfidf_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "# stem_vectorizer = TfidfVectorizer(min_df=15, analyzer = stemmed_words)\n",
    "\n",
    "\n",
    "# Transform X_train\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break the above code cell into steps and show what steps are doing what and why are we doing this overall\n",
    "\n",
    "- The function `build_analyzer()` of `CountVectorizer()` handles the pre-processing, tokenizing and n-grams generation for the text\n",
    "- In the function `stemmed_words()` \n",
    "  - The text is first passed through the `build_analyzer()` and then each word in the text is stemmed to its base form\n",
    "- This whole thing is called with the last step when we call `fit_transform()` on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the below cell we can see how `build_analyzer()` pre-processes the sample text and tokenize it\n",
    "- And at the last line, the stemmer stems each word in the text to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text -  Its going good...no problem..but still need little experience to understand american customer voice...\n",
      "------------------------------\n",
      "Text after passing through build_analyzer -  ['its', 'going', 'good', 'no', 'problem', 'but', 'still', 'need', 'little', 'experience', 'to', 'understand', 'american', 'customer', 'voice']\n",
      "------------------------------\n",
      "Text after stemming -  ['it', 'go', 'good', 'no', 'problem', 'but', 'still', 'need', 'littl', 'experi', 'to', 'understand', 'american', 'custom', 'voic']\n"
     ]
    }
   ],
   "source": [
    "sample_text = X_train[:1]\n",
    "print(\"Sample Text - \", sample_text[872])\n",
    "print(\"-\"*30)\n",
    "print(\"Text after passing through build_analyzer - \", cv_analyzer(sample_text[872]))\n",
    "print(\"-\"*30)\n",
    "print(\"Text after stemming - \",[stemmer.stem(w) for w in cv_analyzer(sample_text[872])])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try uncommenting the tfidf lines in the cell above, so instead of using CountVectorizer you can also use TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_stemm = LogisticRegression(max_iter=1500)\n",
    "model_stemm.fit(X_train_stem_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model_stemm.predict_proba(stem_vectorizer.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'hope' 'he' 'that']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'rington' 'text' 'chat' 'new' 'repli' 'won' 'call' 'cost']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(stem_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_stemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some other words in the features with absolute highest coefficients.\n",
    "The AUC-score of classification is between the scores of our last two text representation attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The same way we used stemming we can also apply lemmatization to our data.\n",
    "Lemmatization reduces variant forms to base form (eg. am, are, is --> be; car, cars, car's, cars' --> car).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# cv_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "# lemm_vectorizer = TfidfVectorizer(min_df=15, analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7091)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemm_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization we were able to reduce the features from ca. 7500 to 7100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.909\n",
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_lemm = LogisticRegression(max_iter=1500)\n",
    "model_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predict_probab = model_lemm.predict_proba(lemm_vectorizer.transform(X_test))[:,1]\n",
    "predictions = model_lemm.predict(lemm_vectorizer.transform(X_test))\n",
    "\n",
    "print(\"F1 = {:.3f}\".format(f1_score(y_test, predictions)))\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'then' 'amp']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'new' 'reply' 'chat' 'won' 'free']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(lemm_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_lemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for this last model are very similar to the first model we tested. \n",
    "\n",
    "You can test how well lemmatization in combination with tf-idf is working on our example data. Just remove the `#` at the beginning of the line (don't forget to add `#` to the respective same lines before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our spam classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our classifier. You can also input your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We went yesterday to the beach, call me first. But then also call the other guy pls. Use this number and then the other number'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your input below\n",
    "input_text = \"We went yesterday to the beach, call me first. But then also call the other guy pls. Use this number and then the other number\"\n",
    "\n",
    "# Or use an example for the test set\n",
    "#input_text = X_test.sample(1).iloc[0]\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a spam :)\n"
     ]
    }
   ],
   "source": [
    "# You can change the model with model_stemm or model_lemm \n",
    "if model.predict(vect.transform([input_text]))[0] == 1:\n",
    "    print('This is a spam!')\n",
    "else:\n",
    "    print('Not a spam :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hamming loss is 0.0230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print(f'The hamming loss is {hamming_loss(y_test,predictions):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to test your own SMS messages and see which words you can add to change the prediction of a ham message to a spam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
